#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M√≥dulo de Processamento e Extra√ß√£o de Leads
Processa os resultados de uma busca massiva REAL do RealSearchOrchestrator
para extrair dados de leads (nomes, emails, perfis) e salv√°-los localmente.
"""

import os
import logging
import asyncio
import time
import json
import csv
import re
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
from urllib.parse import urlparse
# Importa o orquestrador real
from services.real_search_orchestrator import real_search_orchestrator

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Fun√ß√µes de Extra√ß√£o de Leads ---

def extract_lead_data_from_item(item: Dict[str, Any], source_url: str) -> List[Dict[str, str]]:
    """
    Extrai dados de leads de um item de resultado de busca.
    Tenta extrair de snippet, t√≠tulo e conte√∫do bruto.
    """
    leads = []
    combined_text = f"{item.get('title', '')} {item.get('snippet', '')} {item.get('content', '')}".strip()

    if not combined_text:
        return []

    # Padr√£o para emails
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails_found = list(set(re.findall(email_pattern, combined_text))) # Remove duplicatas

    # Padr√£o mais robusto para nomes pr√≥prios brasileiros/portugueses
    # Procura por "Nome Sobrenome" ou "Nome S. Sobrenome"
    # Este √© um padr√£o simplificado, NLP seria mais preciso.
    name_pattern = r'\b[A-Z√Ä-√ñ√ò-√∂√∏-√ø][a-z√†-√∂√∏-√ø]+\s+[A-Z√Ä-√ñ√ò-√∂√∏-√ø][a-z√†-√∂√∏-√ø]+(?:\s+[A-Z√Ä-√ñ√ò-√∂√∏-√ø][a-z√†-√∂√∏-√ø]+)?\b'
    names_found = list(set(re.findall(name_pattern, combined_text))) # Remove duplicatas

    # Heur√≠stica: Se encontrou emails, associa a nomes pr√≥ximos ou usa o t√≠tulo da p√°gina
    if emails_found:
        for email in emails_found:
            # Tenta encontrar um nome associado ao email
            # Procura nome dentro de uma janela de caracteres
            email_pos = combined_text.find(email)
            start = max(0, email_pos - 100)
            end = min(len(combined_text), email_pos + len(email) + 100)
            context_around_email = combined_text[start:end]
            
            nearby_names = re.findall(name_pattern, context_around_email)
            lead_name = nearby_names[0] if nearby_names else item.get('title', 'N/A').split(' - ')[0] # Parte antes do '-'

            leads.append({
                'source_url': source_url,
                'full_name': lead_name,
                'email': email,
                'extracted_from': 'email_context_or_title',
                'extracted_at': datetime.now().isoformat()
            })
    elif names_found:
        # Se s√≥ encontrou nomes, cria leads com nome e email 'N/A'
        for name in names_found[:3]: # Limita para evitar polui√ß√£o
             leads.append({
                'source_url': source_url,
                'full_name': name,
                'email': 'N/A',
                'extracted_from': 'text_content',
                'extracted_at': datetime.now().isoformat()
            })

    if leads:
        logger.debug(f"  üéØ Encontrados {len(leads)} leads potenciais em {source_url}")
    return leads

# --- Fun√ß√µes de Salvamento Local ---

def save_leads_locally(leads: List[Dict[str, str]], session_id: str, query: str):
    """
    Salva os leads coletados em arquivos locais (JSON e CSV).
    Tamb√©m salva um relat√≥rio da sess√£o de coleta de leads.
    """
    if not leads:
        logger.info("üì≠ Nenhum lead coletado para salvar.")
        return

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_query = re.sub(r'[^\w\s-]', '', query).strip().replace(' ', '_')[:50] # Sanitiza a query para nome de arquivo
    
    base_filename = f"leads_{safe_query}_{session_id}_{timestamp}"

    # 1. Salvar em JSON
    json_filename = f"{base_filename}.json"
    try:
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(leads, f, indent=4, ensure_ascii=False)
        logger.info(f"üíæ {len(leads)} leads salvos em {json_filename}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao salvar leads em JSON: {e}")

    # 2. Salvar em CSV
    csv_filename = f"{base_filename}.csv"
    if leads:
        fieldnames = leads[0].keys()
        try:
            with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(leads)
            logger.info(f"üíæ {len(leads)} leads salvos em {csv_filename}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar leads em CSV: {e}")

    # 3. Salvar relat√≥rio da sess√£o de extra√ß√£o de leads
    report_filename = f"{base_filename}_lead_report.json"
    report_data = {
        'session_id': session_id,
        'original_query': query,
        'leads_extracted': len(leads),
        'unique_emails': len(set(lead['email'] for lead in leads if lead['email'] != 'N/A')),
        'unique_names': len(set(lead['full_name'] for lead in leads if lead['full_name'] != 'N/A')),
        'timestamp': datetime.now().isoformat()
    }
    try:
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=4, ensure_ascii=False)
        logger.info(f"üìà Relat√≥rio da extra√ß√£o de leads salvo em {report_filename}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao salvar relat√≥rio de leads: {e}")


# --- Fun√ß√£o Principal de Processamento de Leads ---

async def process_leads_from_massive_search(
    query: str,
    context: Optional[Dict[str, Any]] = None,
    session_id: str = None,
    massive_data: Dict[str, Any] = None
) -> List[Dict[str, Any]]:
    """
    Etapa principal: Executa uma busca massiva REAL e processa os resultados para extrair leads.
    
    Esta fun√ß√£o:
    1. Chama o RealSearchOrchestrator para fazer a busca massiva.
    2. Processa os resultados retornados.
    3. Extrai dados de leads (nomes, emails) de cada item.
    4. Salva os leads localmente.
    5. Retorna a lista de leads extra√≠dos.
    
    :param query: A query de busca (ex: "designers gr√°ficos em S√£o Paulo")
    :param context: Dicion√°rio com contexto adicional para o orquestrador (opcional)
    :return: Lista de dicion√°rios representando os leads coletados
    """
    # 1. Usa dados massivos j√° coletados ou executa nova busca
    if massive_data and isinstance(massive_data, dict):
        search_data = massive_data
    else:
        search_data = await real_search_orchestrator.execute_massive_real_search(
            query=query,
            context=context or {},
            session_id=session_id or f"leads_{int(time.time())}"
        )

    if not search_data or not search_data.get('web_intelligence'):
        logger.error("‚ùå Falha na busca massiva ou nenhum resultado retornado.")
        return []

    web_intel = search_data['web_intelligence']
    
    # Coleta todos os resultados de diferentes fontes
    all_items = []
    all_items.extend(web_intel.get('primary_search', []))
    # Adiciona resultados de queries expandidas
    for expanded_query, results_dict in web_intel.get('expanded_queries', {}).items():
        all_items.extend(results_dict.get('results', []))
    # Adiciona conte√∫do profundo
    all_items.extend(web_intel.get('deep_content', {}).get('extracted_data', []))
    # Adiciona resultados de redes sociais
    all_items.extend(web_intel.get('social_media_insights', {}).get('top_profiles', []))
    
    if not all_items:
         logger.warning("‚ö†Ô∏è Nenhum item encontrado para extra√ß√£o de leads.")
         return []

    logger.info(f"üìä Total de itens para processar: {len(all_items)}")

    # --- ETAPA 2: EXTRA√á√ÉO DE LEADS ---
    logger.info("üìñ ETAPA 2: Extraindo dados de leads dos itens coletados...")
    
    all_extracted_leads = []
    for item in all_items:
        url = item.get('url', 'N/A')
        # Extrai leads de cada item
        leads_from_item = extract_lead_data_from_item(item, url)
        all_extracted_leads.extend(leads_from_item)

    if not all_extracted_leads:
        logger.warning("üì≠ Nenhum lead p√¥de ser extra√≠do dos itens coletados.")
        # Mesmo assim, salva um relat√≥rio vazio para registrar a tentativa
        save_leads_locally([], session_id or f"leads_{int(time.time())}", query)
        return []

    # --- ETAPA 3: REMO√á√ÉO DE DUPLICATAS E FINALIZA√á√ÉO ---
    logger.info("üßπ ETAPA 3: Removendo duplicatas e finalizando...")
    
    # Remove duplicatas baseadas em email (prioriza o primeiro encontrado)
    seen_emails: Set[str] = set()
    unique_leads: List[Dict[str, str]] = []
    duplicates_removed = 0

    for lead in all_extracted_leads:
        email = lead.get('email', 'N/A')
        # Se o lead tem um email e ele j√° foi visto, √© duplicado
        if email != 'N/A' and email in seen_emails:
            duplicates_removed += 1
            continue
        # Se o lead tem um email, marca como visto
        if email != 'N/A':
            seen_emails.add(email)
        # Adiciona lead (√∫nico por email, ou sem email)
        unique_leads.append(lead)

    logger.info(f"‚úÖ Extra√ß√£o conclu√≠da. Leads √∫nicos encontrados: {len(unique_leads)} (Duplicatas removidas: {duplicates_removed})")

    # --- ETAPA 4: SALVAMENTO LOCAL ---
    logger.info("üíæ ETAPA 4: Salvando leads localmente...")
    save_leads_locally(unique_leads, session_id or f"leads_{int(time.time())}", query)
    
    # --- ETAPA 5: RELAT√ìRIO FINAL ---
    logger.info("üìà ETAPA 5: Gerando relat√≥rio final...")
    orchestrator_stats = real_search_orchestrator.get_session_statistics()
    lead_report_data = {
        'session_id': session_id or f"leads_{int(time.time())}",
        'original_query': query,
        'orchestrator_stats': orchestrator_stats,
        'leads_extracted': len(unique_leads),
        'unique_emails': len(set(lead['email'] for lead in unique_leads if lead['email'] != 'N/A')),
        'unique_names': len(set(lead['full_name'] for lead in unique_leads if lead['full_name'] != 'N/A')),
        'timestamp': datetime.now().isoformat()
    }
    
    final_report_filename = f"leads_{session_id or f'leads_{int(time.time())}'}_FINAL_REPORT.json"
    try:
        with open(final_report_filename, 'w', encoding='utf-8') as f:
            json.dump(lead_report_data, f, indent=4, ensure_ascii=False)
        logger.info(f"üèÅ Relat√≥rio final completo salvo em {final_report_filename}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao salvar relat√≥rio final: {e}")

    logger.info(f"üéâ Processo de coleta de leads conclu√≠do com sucesso!")
    return unique_leads


# --- Fun√ß√£o para Execu√ß√£o Independente ---

async def main():
    """Fun√ß√£o principal para demonstrar o uso."""
    
    # 1. Defina sua query de busca
    search_query = "freelancers designers gr√°ficos em porto alegre"
    
    # 2. (Opcional) Defina um contexto para a busca
    search_context = {
        # 'segmento': 'design',
        # 'publico': 'freelancers',
        # 'problema': 'busca por trabalho'
        # O contexto √© usado pelo RealSearchOrchestrator para expandir queries
    }
    
    # 3. Certifique-se de que suas chaves de API est√£o configuradas
    # como vari√°veis de ambiente (veja resposta anterior sobre quais s√£o necess√°rias)
    # Ex: os.environ['SERPER_API_KEY'] = 'sua_chave'
    
    # 4. Execute o processo de coleta de leads
    leads = await process_leads_from_massive_search(
        query=search_query,
        context=search_context
    )
    
    # 5. (Opcional) Imprime um resumo no console
    if leads:
        print(f"\n--- Resumo da Coleta de Leads ---")
        print(f"Query: {search_query}")
        print(f"Leads Encontrados: {len(leads)}")
        # Mostra os 5 primeiros
        for lead in leads[:5]:
            print(f"  - Nome: {lead.get('full_name')}, Email: {lead.get('email')}, Fonte: {lead.get('source_url')}")
        print("---")
    else:
        print("Nenhum lead foi encontrado.")

# --- Objeto de Servi√ßo para Compatibilidade ---

class LeadsService:
    """Classe de servi√ßo para compatibilidade com imports existentes"""
    
    @staticmethod
    async def extract_comprehensive_leads(session_id: str) -> Dict[str, Any]:
        """Extrai leads de forma abrangente para uma sess√£o"""
        try:
            # Carrega dados da sess√£o
            session_dir = f"analyses_data/{session_id}"
            massive_data = None
            
            if os.path.exists(session_dir):
                for file in os.listdir(session_dir):
                    if "massive_search" in file and file.endswith('.json'):
                        with open(os.path.join(session_dir, file), 'r', encoding='utf-8') as f:
                            massive_data = json.load(f)
                        break
            
            # Processa leads
            leads_extracted = await process_leads_from_massive_search(
                query="comprehensive leads extraction",
                context={"session_id": session_id},
                session_id=session_id,
                massive_data=massive_data
            )
            
            return {
                "leads": leads_extracted,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o abrangente de leads: {e}")
            return {"leads": [], "error": str(e)}

# Inst√¢ncia global do servi√ßo
leads_service = LeadsService()

if __name__ == "__main__":
    # Executa a fun√ß√£o principal
    asyncio.run(main())